Решались две задачи MountainCar-V0 и MountainCarContinuous-v0. В ноутбуке MountainCar-v0.ipynb решение первой, в ActorCritic.ipynb - второй.

Для MountainCar-V0 использовался DQN алгоритм, основанный на Q-функции. Его преимущество по сравнению с Policy gradient в том, что можно использовать старые данные для обучения. 
1) Создаётся класс Policy на основе модуля torch.nn с двумя линейными слоями(без нелинейности между ними, хотя можно и добавить). На выходе вероятности для каждого их трёх действий.
2) ReplayMemory сохраняет данные в виде Transition и перезаписывает, если объём памяти превышен.
3) Создаётся две нейросети policy_net и target_net. Первая будет обновляться на каждом шаге, вторая через TARGET_UPDATE итераций. target_net используется в расчёте функции потерь для Q-функции.
4) Действие выбирается либо исходя из максимума вероятности, выданной нейросетью, либо случайно. Вероятность случайного выбора уменьшается при увеличении числа итераций.
5) optimize_model() берёт batch данных и осуществляет обратный проход. Функция потерь считается, как MSE между выходом нейросети(текущей версий Q-функции) и ожидаемым максимальным reward'ом, рассчитываемым с помощью target_net(старой версией Q-функции). При чём используются результаты Q-функции для всех возможных действий и соответственно всех возможных следующих состояний. Это возможно, потому что их всего три.
6) В эпизодах запись всех трёх возможных следующих состояний осуществляется с помощью перезаписи состояний среды.
7)Результат - 100% на 50 шагах. Если запускать большее число раз, то ошибки всё же будут.

Для MountainCarContinuous-v0 была попытка сделать обычный Policy gradient, где выход нейросети - параметры нормального распределения. Но из-за того, что в PG подходе нельзя менять распределение вероятности выбираемых действий в обход функции потерь(тогда градиенты будут считаться неправильно) мне не удалось добиться того, чтобы в начале достижение финиша случалось достаточно часто. И в такой постановке результатом обучения было отсутствие каких-либо движений.

Второй попыткой стал ActorCritic подход.
1) Задаются нейросети actor и critic, первый выдаёт само действие(просто число), второй пытается предсказать reward на основе состояния среды и действия.
2) В тренировке используется MSE для сравнения результата критика с reward'ом. А для оптимизации весов actor'а используется функция потерь равная сумме результатов критика на состояние среды и действие актора на данное состояние с минусом. 
Таким образом, если критик научится хорошо предсказывать reward, то качество обучения актора не будет зависеть от того в какой момент были получены данные, на которых он учится.
Также как и обучение критика - ему нужен лишь набор state, action, reward.
3) Для начала нужно натренировать сеть взбираться на горку, как в первой задаче. Для этого можно делать случайное действие, а затем тренировать сеть с discounted rewards со значениями из первой задачи(для всех -1, для вершины +100). Если сеть натренировалась так, что взбирается на горку хорошо, то можно перейти к следующему пункту.
4) Здесь в шуме уже нет необходимости, нужно лишь выбирать действие с весами натренированными в предыдущем пункте и обучать сеть с настоящими rewards.
5) Хорошего результата, к сожалению, пока нет. Для правильного подбора параметров для такой нейросети нужно много времени, которого у меня по ряду обстоятельств не было.
